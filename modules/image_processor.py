"""
å›¾åƒå¤„ç†æ¨¡å—
åŒ…å«çº¿æ¡æå–ç®—æ³•ï¼šPencil Sketch, Canny, Anime2Sketch
"""

import cv2
import numpy as np
from scipy.interpolate import splprep, splev
from modules.smart_resize import adaptive_resize, calculate_optimal_size

try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("è­¦å‘Š: PyTorch æœªå®‰è£…ï¼ŒAnime2Sketch åŠŸèƒ½å°†ä¸å¯ç”¨")


# --- Anime2Sketch æ¨¡å‹å®šä¹‰ ---
if TORCH_AVAILABLE:
    class UnetGenerator(nn.Module):
        def __init__(self, input_nc=3, output_nc=1, num_downs=8, ngf=64):
            super(UnetGenerator, self).__init__()
            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=nn.InstanceNorm2d, innermost=True)
            for i in range(num_downs - 5):
                unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=nn.InstanceNorm2d, use_dropout=True)
            unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=nn.InstanceNorm2d)
            unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=nn.InstanceNorm2d)
            unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=nn.InstanceNorm2d)
            self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=nn.InstanceNorm2d)

        def forward(self, input):
            return self.model(input)

    class UnetSkipConnectionBlock(nn.Module):
        def __init__(self, outer_nc, inner_nc, input_nc=None, submodule=None, outermost=False, innermost=False, norm_layer=nn.InstanceNorm2d, use_dropout=False):
            super(UnetSkipConnectionBlock, self).__init__()
            self.outermost = outermost
            if input_nc is None:
                input_nc = outer_nc
            downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4, stride=2, padding=1, bias=False)
            downrelu = nn.LeakyReLU(0.2, True)
            downnorm = norm_layer(inner_nc)
            uprelu = nn.ReLU(True)
            upnorm = norm_layer(outer_nc)

            if outermost:
                upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1)
                down = [downconv]
                up = [uprelu, upconv, nn.Tanh()]
                model = down + [submodule] + up
            elif innermost:
                upconv = nn.ConvTranspose2d(inner_nc, outer_nc, kernel_size=4, stride=2, padding=1, bias=False)
                down = [downrelu, downconv]
                up = [uprelu, upconv, upnorm]
                model = down + up
            else:
                upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1, bias=False)
                down = [downrelu, downconv, downnorm]
                up = [uprelu, upconv, upnorm]
                if use_dropout:
                    model = down + [submodule] + up + [nn.Dropout(0.5)]
                else:
                    model = down + [submodule] + up

            self.model = nn.Sequential(*model)

        def forward(self, x):
            if self.outermost:
                return self.model(x)
            else:
                return torch.cat([x, self.model(x)], 1)


# --- å…¨å±€æ¨¡å‹ç¼“å­˜ ---
anime2sketch_model = None


def get_anime2sketch_model():
    """åŠ è½½æˆ–è·å–ç¼“å­˜çš„ Anime2Sketch æ¨¡å‹"""
    global anime2sketch_model
    
    if anime2sketch_model is not None:
        return anime2sketch_model
    
    if not TORCH_AVAILABLE:
        print("é”™è¯¯: PyTorch æœªå®‰è£…")
        anime2sketch_model = "error"
        return None
    
    if anime2sketch_model == "error":
        return None
    
    # å°è¯•åŠ è½½æ¨¡å‹
    try:
        import os
        # æ¨¡å‹è·¯å¾„ï¼šä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œæ”¯æŒä¸­æ–‡è·¯å¾„
        script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        model_path = os.path.join(script_dir, "models", "netG.pth")
        
        # ç¡®ä¿è·¯å¾„å­˜åœ¨
        if not os.path.exists(model_path):
            # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
            alternative_paths = [
                os.path.join(os.getcwd(), "models", "netG.pth"),
                os.path.join(script_dir, "netG.pth"),
                "models/netG.pth",
                "netG.pth"
            ]
            
            for alt_path in alternative_paths:
                if os.path.exists(alt_path):
                    model_path = alt_path
                    print(f"æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
                    break
            else:
                # æ‰€æœ‰è·¯å¾„éƒ½ä¸å­˜åœ¨
                print(f"=" * 60)
                print(f"é”™è¯¯: æ‰¾ä¸åˆ° Anime2Sketch æ¨¡å‹æ–‡ä»¶")
                print(f"å°è¯•çš„è·¯å¾„:")
                print(f"  1. {os.path.join(script_dir, 'models', 'netG.pth')}")
                for i, p in enumerate(alternative_paths, 2):
                    print(f"  {i}. {p}")
                print(f"")
                print(f"è¯·ä¸‹è½½æ¨¡å‹æ–‡ä»¶:")
                print(f"  ä¸‹è½½åœ°å€: https://github.com/Mukosame/Anime2Sketch/releases/download/1.0/netG.pth")
                print(f"  æ”¾ç½®ä½ç½®: {os.path.join(script_dir, 'models', 'netG.pth')}")
                print(f"=" * 60)
                anime2sketch_model = "error"
                return None
        
        model_path = os.path.abspath(model_path)
        
        if not os.path.exists(model_path):
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
            print("è¯·ä¸‹è½½ Anime2Sketch æ¨¡å‹å¹¶æ”¾ç½®åˆ° models æ–‡ä»¶å¤¹:")
            print("https://github.com/Mukosame/Anime2Sketch/releases/download/1.0/netG.pth")
            anime2sketch_model = "error"
            return None
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        anime2sketch_model = UnetGenerator(input_nc=3, output_nc=1, num_downs=8, ngf=64)
        
        # åŠ è½½æƒé‡å¹¶å¤„ç† 'module.' å‰ç¼€
        state_dict = torch.load(model_path, map_location=device)
        
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith('module.'):
                new_state_dict[k[7:]] = v
            else:
                new_state_dict[k] = v
        
        missing_keys, unexpected_keys = anime2sketch_model.load_state_dict(new_state_dict, strict=False)
        
        if missing_keys:
            print(f"è­¦å‘Š: ç¼ºå°‘çš„é”®: {len(missing_keys)} ä¸ª")
        if unexpected_keys:
            print(f"è­¦å‘Š: æ„å¤–çš„é”®: {len(unexpected_keys)} ä¸ª (å·²å¿½ç•¥)")
        
        anime2sketch_model.to(device)
        anime2sketch_model.eval()
        print(f"Anime2Sketch æ¨¡å‹åŠ è½½æˆåŠŸ (è®¾å¤‡: {device})")
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•åŠ è½½ Anime2Sketch æ¨¡å‹: {e}")
        import traceback
        traceback.print_exc()
        anime2sketch_model = "error"
        return None
    
    return anime2sketch_model


# --- å›¾åƒå¤„ç†å‡½æ•° ---

def load_image_with_unicode(file_path, target_size=None):
    """
    æ”¯æŒ Unicode è·¯å¾„çš„å›¾ç‰‡åŠ è½½å¹¶è‡ªåŠ¨è°ƒæ•´åˆ°åˆç†å°ºå¯¸
    
    å‚æ•°:
        file_path: å›¾ç‰‡è·¯å¾„ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰
        target_size: ç›®æ ‡å°ºå¯¸ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœæä¾›åˆ™ä¼˜å…ˆä½¿ç”¨
    
    è¿”å›:
        image: BGR å›¾åƒæ•°ç»„
    """
    try:
        # ä½¿ç”¨ numpy è¯»å–æ–‡ä»¶å­—èŠ‚ï¼Œç„¶åç”¨ cv2 è§£ç 
        with open(file_path, 'rb') as f:
            file_bytes = np.frombuffer(f.read(), dtype=np.uint8)
        image = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
        
        if image is None:
            print(f"é”™è¯¯: æ— æ³•è§£ç å›¾åƒæ–‡ä»¶: {file_path}")
            print(f"  è¯·ç¡®è®¤æ–‡ä»¶æ˜¯æœ‰æ•ˆçš„å›¾ç‰‡æ ¼å¼ï¼ˆjpg, png, bmp ç­‰ï¼‰")
            return None
        
        # è·å–åŸå§‹å°ºå¯¸
        h, w = image.shape[:2]
        original_size = f"{w}x{h}"
        
        # æ™ºèƒ½ç¼©æ”¾ç­–ç•¥ï¼šä½¿ç”¨é«˜è´¨é‡ç®—æ³•é˜²æ­¢ç²¾åº¦æŸå¤±
        MAX_SIZE = 2048       # ç»å¯¹ä¸Šé™ï¼ˆé˜²æ­¢å†…å­˜é—®é¢˜ï¼‰
        MIN_SIZE = 256        # ç»å¯¹ä¸‹é™
        DEFAULT_TARGET = 1024 # é»˜è®¤ç›®æ ‡å°ºå¯¸
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®æ ‡å°ºå¯¸ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥
        if target_size is None:
            target_size = DEFAULT_TARGET
        
        max_dim = max(h, w)
        
        # æ™ºèƒ½ç¼©æ”¾é€»è¾‘ï¼ˆä½¿ç”¨é«˜è´¨é‡ç®—æ³•ï¼‰
        if max_dim > MAX_SIZE:
            # è¶…å¤§å›¾ç‰‡ï¼šä½¿ç”¨æ™ºèƒ½ä¸‹é‡‡æ ·
            scale = MAX_SIZE / max_dim
            new_w = int(w * scale)
            new_h = int(h * scale)
            print(f"ğŸ”§ å›¾ç‰‡è¿‡å¤§ ({original_size})ï¼Œä½¿ç”¨æ™ºèƒ½ç®—æ³•ç¼©å°...")
            image = adaptive_resize(image, new_w, new_h, preserve_detail=True)
            print(f"   âœ“ å·²ä¼˜åŒ–åˆ° {new_w}x{new_h} (ä¿ç•™ç»†èŠ‚)")
            
        elif max_dim > target_size * 1.5:
            # å›¾ç‰‡è¾ƒå¤§ï¼šä½¿ç”¨æ™ºèƒ½ä¸‹é‡‡æ ·
            scale = target_size / max_dim
            new_w = int(w * scale)
            new_h = int(h * scale)
            print(f"ğŸ”§ å›¾ç‰‡è¾ƒå¤§ ({original_size})ï¼Œä½¿ç”¨æ™ºèƒ½ç®—æ³•ä¼˜åŒ–...")
            image = adaptive_resize(image, new_w, new_h, preserve_detail=True)
            print(f"   âœ“ å·²ä¼˜åŒ–åˆ° {new_w}x{new_h} (é˜²æ­¢ç²¾åº¦æŸå¤±)")
            
        elif max_dim < MIN_SIZE:
            # å›¾ç‰‡å¤ªå°ï¼šä½¿ç”¨æ™ºèƒ½ä¸Šé‡‡æ ·
            scale = MIN_SIZE / max_dim
            new_w = int(w * scale)
            new_h = int(h * scale)
            print(f"ğŸ”§ å›¾ç‰‡è¿‡å° ({original_size})ï¼Œä½¿ç”¨æ™ºèƒ½ç®—æ³•æ”¾å¤§...")
            image = adaptive_resize(image, new_w, new_h, preserve_detail=True)
            print(f"   âœ“ å·²æ”¾å¤§åˆ° {new_w}x{new_h} (è¾¹ç¼˜ä¿ç•™)")
        
        else:
            # å°ºå¯¸åˆç†ï¼šä¿æŒåŸæ ·
            print(f"âœ… å›¾ç‰‡å°ºå¯¸åˆé€‚: {original_size}")
        
        return image
    except FileNotFoundError:
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None
    except PermissionError:
        print(f"é”™è¯¯: æ²¡æœ‰è¯»å–æƒé™: {file_path}")
        return None
    except Exception as e:
        print(f"é”™è¯¯: åŠ è½½å›¾åƒå¤±è´¥: {file_path}")
        print(f"  å¼‚å¸¸ä¿¡æ¯: {e}")
        return None


def process_image_pencil(file_path, sigma_s, sigma_r, shade_factor, 
                         simplify_eps, spline_smoothness, preview_thickness):
    """Pencil Sketch çº¿æ¡æå–"""
    try:
        image_rgb = load_image_with_unicode(file_path)
        if image_rgb is None:
            print("é”™è¯¯: æ— æ³•åŠ è½½å›¾åƒ")
            return None, None, 0, 0
        
        img_gray, img_blur = cv2.pencilSketch(
            image_rgb,
            sigma_s=sigma_s,
            sigma_r=sigma_r,
            shade_factor=shade_factor
        )
        
        _, edges = cv2.threshold(img_gray, 240, 255, cv2.THRESH_BINARY_INV)
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        simplified_contours = [cv2.approxPolyDP(c, simplify_eps, True) for c in contours]
        
        final_contours = simplified_contours
        if spline_smoothness > 0:
            final_contours = smooth_contours(simplified_contours, spline_smoothness)
        
        preview_canvas = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2BGR)
        for contour in final_contours:
            cv2.polylines(preview_canvas, [contour], False, (0, 0, 255), int(preview_thickness), lineType=cv2.LINE_AA)
        
        total_points = sum(len(c) for c in final_contours)
        print(f"(Pencil) é¢„è§ˆ: {len(final_contours)} æ¡è·¯å¾„, {total_points} ä¸ªç‚¹ã€‚")
        
        return preview_canvas, final_contours, image_rgb.shape[1], image_rgb.shape[0]
    
    except Exception as e:
        print(f"Pencil Sketch å›¾åƒå¤„ç†é”™è¯¯: {e}")
        return None, None, 0, 0


def process_image_canny(file_path, blur_kernel, low_thresh, high_thresh,
                        simplify_eps, spline_smoothness, preview_thickness):
    """Canny è¾¹ç¼˜æ£€æµ‹"""
    try:
        image_rgb = load_image_with_unicode(file_path)
        if image_rgb is None:
            print("é”™è¯¯: æ— æ³•åŠ è½½å›¾åƒ")
            return None, None, 0, 0
        
        img_gray = cv2.cvtColor(image_rgb, cv2.COLOR_BGR2GRAY)
        img_blur = cv2.GaussianBlur(img_gray, (blur_kernel, blur_kernel), 0)
        edges = cv2.Canny(img_blur, low_thresh, high_thresh)
        
        contours, _ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
        simplified_contours = [cv2.approxPolyDP(c, simplify_eps, True) for c in contours]
        
        final_contours = simplified_contours
        if spline_smoothness > 0:
            final_contours = smooth_contours(simplified_contours, spline_smoothness)
        
        preview_canvas = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
        for contour in final_contours:
            cv2.polylines(preview_canvas, [contour], False, (0, 0, 255), int(preview_thickness), lineType=cv2.LINE_AA)
        
        total_points = sum(len(c) for c in final_contours)
        print(f"(Canny) é¢„è§ˆ: {len(final_contours)} æ¡è·¯å¾„, {total_points} ä¸ªç‚¹ã€‚")
        
        return preview_canvas, final_contours, image_rgb.shape[1], image_rgb.shape[0]
    
    except Exception as e:
        print(f"Canny å›¾åƒå¤„ç†é”™è¯¯: {e}")
        return None, None, 0, 0


def process_image_anime2sketch(file_path, simplify_eps, spline_smoothness, preview_thickness, 
                                threshold_val, morph_size, morph_iter, min_area, contour_mode="å¤–éƒ¨è½®å»“ (å¿«é€Ÿ)",
                                pre_blur=0, edge_enhance=0, sigmoid_threshold=0.5, invert=False, adaptive=False):
    """Anime2Sketch AI çº¿æ¡æå–"""
    try:
        if not TORCH_AVAILABLE:
            error_img = np.zeros((300, 500, 3), dtype=np.uint8)
            cv2.putText(error_img, "PyTorch Not Installed", (50, 150), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
            return error_img, [], 500, 300
        
        image_rgb = load_image_with_unicode(file_path)
        if image_rgb is None:
            print("é”™è¯¯: æ— æ³•åŠ è½½å›¾åƒ")
            return None, None, 0, 0
        
        model = get_anime2sketch_model()
        if model is None:
            error_img = np.zeros((300, 500, 3), dtype=np.uint8)
            cv2.putText(error_img, "Model Failed to Load", (50, 150), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
            return error_img, [], 500, 300
        
        # é¢„å¤„ç†å›¾åƒ
        h, w = image_rgb.shape[:2]
        original_h, original_w = h, w  # ä¿å­˜åŸå§‹å°ºå¯¸
        target_h = ((h - 1) // 256 + 1) * 256
        target_w = ((w - 1) // 256 + 1) * 256
        
        max_size = 1024
        need_restore_size = False  # ğŸ”§ æ ‡å¿—:æ˜¯å¦éœ€è¦æ¢å¤åŸå§‹å°ºå¯¸
        
        if target_h > max_size or target_w > max_size:
            need_restore_size = True  # æ ‡è®°éœ€è¦æ¢å¤
            scale = min(max_size / target_h, max_size / target_w)
            new_h = int(h * scale)
            new_w = int(w * scale)
            image_resized = cv2.resize(image_rgb, (new_w, new_h))
            target_h = ((new_h - 1) // 256 + 1) * 256
            target_w = ((new_w - 1) // 256 + 1) * 256
        else:
            image_resized = image_rgb
        
        pad_h = target_h - image_resized.shape[0]
        pad_w = target_w - image_resized.shape[1]
        image_padded = cv2.copyMakeBorder(image_resized, 0, pad_h, 0, pad_w, cv2.BORDER_REFLECT)
        
        # è½¬æ¢ä¸ºå¼ é‡
        image = cv2.cvtColor(image_padded, cv2.COLOR_BGR2RGB)
        image = image.astype(np.float32) / 255.0
        image = np.transpose(image, (2, 0, 1))
        image = torch.from_numpy(image).unsqueeze(0).float()
        
        device = next(model.parameters()).device
        image = image.to(device)
        
        # æ¨ç†
        with torch.no_grad():
            edge = model(image)
            edge = torch.sigmoid((edge - sigmoid_threshold) * 10)
        
        # åå¤„ç†
        edge_map = edge.squeeze().cpu().numpy()
        edge_map = edge_map[:image_resized.shape[0], :image_resized.shape[1]]
        
        # ğŸ”§ å…³é”®ä¿®å¤: å…ˆè½¬æ¢åˆ°8ä½å†æ”¾å¤§(é¿å…è½®å»“åæ ‡é”™ä½)
        edge_map = (edge_map * 255.0).astype(np.uint8)
        
        # ğŸ”§ ä½¿ç”¨æ ‡å¿—å˜é‡åˆ¤æ–­æ˜¯å¦éœ€è¦æ¢å¤å°ºå¯¸
        if need_restore_size:
            print(f"  [ç¼©æ”¾æ¢å¤] {image_resized.shape[1]}x{image_resized.shape[0]} â†’ {original_w}x{original_h}")
            edge_map = cv2.resize(edge_map, (original_w, original_h), interpolation=cv2.INTER_LINEAR)
        
        # é¢„å¤„ç†æ¨¡ç³Š
        if pre_blur > 0:
            blur_size = pre_blur if pre_blur % 2 == 1 else pre_blur + 1
            edge_map = cv2.GaussianBlur(edge_map, (blur_size, blur_size), 0)
            print(f"  [é¢„å¤„ç†] é«˜æ–¯æ¨¡ç³Š: {blur_size}x{blur_size}")
        
        # è¾¹ç¼˜å¢å¼º
        if edge_enhance > 0:
            kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]]) * edge_enhance / 3.0
            edge_map = cv2.filter2D(edge_map, -1, kernel)
            edge_map = np.clip(edge_map, 0, 255).astype(np.uint8)
            print(f"  [è¾¹ç¼˜å¢å¼º] å¼ºåº¦: {edge_enhance:.1f}")
        
        # äºŒå€¼åŒ–
        if adaptive:
            binary = cv2.adaptiveThreshold(edge_map, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                          cv2.THRESH_BINARY_INV if not invert else cv2.THRESH_BINARY, 11, 2)
            print(f"  [äºŒå€¼åŒ–] è‡ªé€‚åº”æ–¹æ³•")
        else:
            thresh_type = cv2.THRESH_BINARY_INV if not invert else cv2.THRESH_BINARY
            _, binary = cv2.threshold(edge_map, threshold_val, 255, thresh_type)
            print(f"  [äºŒå€¼åŒ–] å›ºå®šé˜ˆå€¼: {threshold_val}")
        
        white_pixels_before = np.sum(binary == 255)
        
        # å½¢æ€å­¦å¤„ç†
        kernel = np.ones((morph_size, morph_size), np.uint8)
        binary = cv2.dilate(binary, kernel, iterations=morph_iter)
        white_pixels_after = np.sum(binary == 255)
        
        pixel_increase = white_pixels_after - white_pixels_before
        increase_percent = (pixel_increase / white_pixels_before * 100) if white_pixels_before > 0 else 0
        print(f"  [å½¢æ€å­¦] æ ¸:{morph_size}x{morph_size}, è¿­ä»£:{morph_iter} â†’ +{increase_percent:.1f}%")
        
        # æå–è½®å»“
        if "éª¨æ¶æå–" in contour_mode:
            skeleton = cv2.ximgproc.thinning(binary, thinningType=cv2.ximgproc.THINNING_ZHANGSUEN)
            contours, _ = cv2.findContours(skeleton, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
            print(f"  [éª¨æ¶æ¨¡å¼] {len(contours)} æ¡")
        elif "æ‰€æœ‰è½®å»“" in contour_mode:
            contours, _ = cv2.findContours(binary, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
            print(f"  [æ‰€æœ‰è½®å»“] {len(contours)} æ¡")
        else:
            contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            print(f"  [å¤–éƒ¨è½®å»“] {len(contours)} æ¡")
        
        # è¿‡æ»¤
        contours = [c for c in contours if cv2.contourArea(c) > min_area]
        print(f"  è¿‡æ»¤å: {len(contours)} æ¡")
        
        # ç®€åŒ–å’Œå¹³æ»‘
        simplified_contours = [cv2.approxPolyDP(c, simplify_eps, True) for c in contours]
        final_contours = simplified_contours
        if spline_smoothness > 0:
            final_contours = smooth_contours(simplified_contours, spline_smoothness)
        
        # é¢„è§ˆ
        preview_canvas = cv2.cvtColor(edge_map, cv2.COLOR_GRAY2BGR)
        for contour in final_contours:
            cv2.polylines(preview_canvas, [contour], False, (0, 0, 255), int(preview_thickness), lineType=cv2.LINE_AA)
        
        total_points = sum(len(c) for c in final_contours)
        print(f"(Anime2Sketch) {len(final_contours)} æ¡è·¯å¾„, {total_points} ä¸ªç‚¹")
        
        # ğŸ” è°ƒè¯•:æ£€æŸ¥è½®å»“åæ ‡èŒƒå›´
        if len(final_contours) > 0:
            all_points = np.vstack([c.reshape(-1, 2) for c in final_contours])
            min_x, min_y = np.min(all_points, axis=0)
            max_x, max_y = np.max(all_points, axis=0)
            print(f"  [åæ ‡èŒƒå›´] X: {min_x:.0f}~{max_x:.0f} (åº”ä¸º 0~{image_rgb.shape[1]})")
            print(f"             Y: {min_y:.0f}~{max_y:.0f} (åº”ä¸º 0~{image_rgb.shape[0]})")
            
            # æ£€æŸ¥æ˜¯å¦åªå ä¸€å°éƒ¨åˆ†
            x_coverage = (max_x - min_x) / image_rgb.shape[1] * 100
            y_coverage = (max_y - min_y) / image_rgb.shape[0] * 100
            if x_coverage < 50 or y_coverage < 50:
                print(f"  âš ï¸ è­¦å‘Š: è½®å»“åªè¦†ç›–å›¾åƒçš„ {x_coverage:.1f}% (X) Ã— {y_coverage:.1f}% (Y)")
                print(f"           è¿™å¯èƒ½æ˜¯è¾¹ç¼˜æ£€æµ‹å‚æ•°é—®é¢˜æˆ–å›¾ç‰‡å†…å®¹é›†ä¸­åœ¨å±€éƒ¨")
        
        return preview_canvas, final_contours, image_rgb.shape[1], image_rgb.shape[0]
    
    except Exception as e:
        print(f"Anime2Sketch é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None, None, 0, 0


def smooth_contours(contours, smoothness_s, new_points_factor=1.5):
    """B-spline æ ·æ¡å¹³æ»‘"""
    smoothed_contours = []
    
    for c in contours:
        if len(c) < 4:
            smoothed_contours.append(c)
            continue
        
        try:
            x, y = c[:, 0, 0], c[:, 0, 1]
            tck, u = splprep([x, y], s=smoothness_s, k=3, per=0)
            num_new_points = int(len(c) * new_points_factor)
            if num_new_points < 4:
                num_new_points = 4
            u_new = np.linspace(u.min(), u.max(), num_new_points)
            x_new, y_new = splev(u_new, tck)
            new_contour = np.array([x_new, y_new]).T.reshape(-1, 1, 2).astype(np.int32)
            smoothed_contours.append(new_contour)
        except:
            smoothed_contours.append(c)
    
    return smoothed_contours
